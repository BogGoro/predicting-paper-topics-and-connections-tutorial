{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde2871c",
   "metadata": {},
   "source": [
    "# Tutorial: Joint Node Classification and Link Prediction in Citation Networks\n",
    "\n",
    "## Applying Graph Machine Learning to Academic Paper Analysis\n",
    "\n",
    "**Authors**: Denis Troegubov  \n",
    "**Date**: December 2025  \n",
    "**GitHub**: [Link](https://github.com/BogGoro/predicting-paper-topics-and-connections-tutorial)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this hands-on tutorial on Graph Machine Learning! In this notebook, we'll explore how to apply state-of-the-art Graph Neural Networks (GNNs) to solve two fundamental problems in citation networks:\n",
    "\n",
    "1. **Node Classification**: Predicting research topics of academic papers\n",
    "2. **Link Prediction**: Recommending potential citations between papers\n",
    "\n",
    "We'll use the **Cora dataset** - a classic benchmark in graph ML - and implement our models using **PyTorch Geometric (PyG)**, the leading library for deep learning on graphs.\n",
    "\n",
    "### Why This Tutorial?\n",
    "- **Practical Approach**: We focus on real-world applications rather than theory\n",
    "- **Code-First**: Learn by implementing working models\n",
    "- **Production-Ready**: Code structured for reproducibility and extension\n",
    "\n",
    "### Prerequisites\n",
    "- Basic knowledge of Python and PyTorch\n",
    "- Understanding of neural networks\n",
    "- No prior experience with graphs required!\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f523f2c",
   "metadata": {},
   "source": [
    "1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d73e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Geometric if you are running in colab\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import SAGEConv, GCNConv\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10667a9c",
   "metadata": {},
   "source": [
    "## 2. Understanding the Data: Cora Dataset\n",
    "\n",
    "### What is Cora?\n",
    "The Cora dataset is a classic citation network consisting of machine learning papers. It's widely used as a benchmark in graph ML research.\n",
    "\n",
    "### Dataset Statistics:\n",
    "- **Nodes**: 2,708 academic papers\n",
    "- **Edges**: 10,556 citation links (directed)\n",
    "- **Features**: 1,433-dimensional binary word vectors (bag-of-words)\n",
    "- **Classes**: 7 research topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Has self-loops: {data.has_self_loops()}\")\n",
    "print(f\"Is undirected: {data.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class distribution\n",
    "class_names = [\n",
    "    \"Case-Based\",\n",
    "    \"Genetic Algorithms\",\n",
    "    \"Neural Networks\",\n",
    "    \"Probabilistic Methods\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Rule Learning\",\n",
    "    \"Theory\",\n",
    "]\n",
    "\n",
    "class_counts = torch.bincount(data.y).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(class_names, class_counts, color=sns.color_palette(\"husl\", 7))\n",
    "plt.title(\n",
    "    \"Distribution of Paper Topics in Cora Dataset\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Research Topic\", fontsize=12)\n",
    "plt.ylabel(\"Number of Papers\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, class_counts):\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 5,\n",
    "        f\"{count}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/class_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a20da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a subgraph of the citation network\n",
    "def visualize_citation_subgraph(data, num_nodes=100):\n",
    "    \"\"\"Visualize a small subgraph of the citation network\"\"\"\n",
    "    # Take first num_nodes nodes\n",
    "    subgraph_nodes = torch.arange(num_nodes)\n",
    "\n",
    "    # Create mask for edges between these nodes\n",
    "    mask = (data.edge_index[0] < num_nodes) & (data.edge_index[1] < num_nodes)\n",
    "    subgraph_edges = data.edge_index[:, mask]\n",
    "\n",
    "    # Create subgraph\n",
    "    subgraph = Data(\n",
    "        x=data.x[:num_nodes], edge_index=subgraph_edges, y=data.y[:num_nodes]\n",
    "    )\n",
    "\n",
    "    # Convert to NetworkX for visualization\n",
    "    G = to_networkx(subgraph, to_undirected=True)\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Node colors by class\n",
    "    node_colors = [data.y[i].item() for i in range(num_nodes)]\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, node_size=50, node_color=node_colors, cmap=plt.cm.Set2, alpha=0.8\n",
    "    )\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)\n",
    "\n",
    "    plt.title(\n",
    "        f\"Citation Network Subgraph (First {num_nodes} Papers)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Create legend for classes\n",
    "    legend_elements = [\n",
    "        plt.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"w\",\n",
    "            markerfacecolor=plt.cm.Set2(i / 7),\n",
    "            markersize=10,\n",
    "            label=class_names[i],\n",
    "        )\n",
    "        for i in range(7)\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=legend_elements,\n",
    "        title=\"Research Topics\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"images/citation_subgraph.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# Visualize subgraph\n",
    "G = visualize_citation_subgraph(data, num_nodes=2708)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcdc8b",
   "metadata": {},
   "source": [
    "## 3. Graph Neural Networks: Core Concepts\n",
    "\n",
    "### What are GNNs?\n",
    "Graph Neural Networks extend deep learning to graph-structured data. Unlike CNNs for images or RNNs for sequences, GNNs can handle arbitrary graph structures.\n",
    "\n",
    "### Key Idea: Message Passing\n",
    "GNNs work by **aggregating information from neighbors**. Each layer:\n",
    "1. **Gathers** neighbor features\n",
    "2. **Aggregates** them (sum, mean, max)\n",
    "3. **Updates** node representations\n",
    "\n",
    "### Why GraphSAGE?\n",
    "We'll use **GraphSAGE** (SAmple and aggreGatE) because:\n",
    "- **Inductive learning**: Can generalize to unseen nodes\n",
    "- **Scalability**: Works with large graphs via neighborhood sampling\n",
    "- **Flexibility**: Can use different aggregation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f203",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "We'll implement a **joint model** that solves both tasks simultaneously:\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "```\n",
    "JointGraphModel\n",
    "├── GraphSAGE Encoder (Shared)\n",
    "│   ├── SAGEConv Layer 1\n",
    "│   └── SAGEConv Layer 2\n",
    "├── Node Classifier\n",
    "└── Link Predictor\n",
    "```\n",
    "\n",
    "### Training Strategy:\n",
    "1. **Phase 1**: Train for node classification\n",
    "2. **Phase 2**: Train for link prediction using learned embeddings\n",
    "3. **Optionally**: Fine-tune jointly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GraphSAGE encoder\n",
    "class GraphSAGEEncoder(nn.Module):\n",
    "    \"\"\"GraphSAGE encoder for learning node representations\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First SAGE layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second SAGE layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get intermediate node embeddings\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.conv1(x, edge_index)\n",
    "            embeddings = F.relu(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Link Predictor\n",
    "class LinkPredictor(nn.Module):\n",
    "    \"\"\"MLP for predicting links between nodes\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_channels * 2, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        # Get embeddings for source and destination nodes\n",
    "        src = z[edge_index[0]]\n",
    "        dst = z[edge_index[1]]\n",
    "\n",
    "        # Concatenate and predict\n",
    "        x = torch.cat([src, dst], dim=1)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the complete joint model\n",
    "class JointGraphModel(nn.Module):\n",
    "    \"\"\"Joint model for node classification and link prediction\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        # Shared encoder\n",
    "        self.encoder = GraphSAGEEncoder(in_channels, hidden_channels, hidden_channels)\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.node_classifier = nn.Linear(hidden_channels, num_classes)\n",
    "        self.link_predictor = LinkPredictor(hidden_channels, hidden_channels // 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Get node embeddings\n",
    "        z = self.encoder(x, edge_index)\n",
    "\n",
    "        # Node classification\n",
    "        node_logits = self.node_classifier(z)\n",
    "\n",
    "        return z, node_logits\n",
    "\n",
    "    def predict_links(self, z, edge_index):\n",
    "        \"\"\"Predict links given node embeddings\"\"\"\n",
    "        return self.link_predictor(z, edge_index)\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get intermediate embeddings\"\"\"\n",
    "        return self.encoder.get_embeddings(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c45ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = JointGraphModel(\n",
    "    in_channels=data.num_features, hidden_channels=128, num_classes=dataset.num_classes\n",
    ")\n",
    "\n",
    "print(\"Model initialized successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\n",
    "    f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b271b",
   "metadata": {},
   "source": [
    "## 5. Data Preparation for Link Prediction\n",
    "\n",
    "For link prediction, we need to:\n",
    "1. Split edges into train/val/test sets\n",
    "2. Create negative examples (non-existent edges)\n",
    "3. Balance positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_link_prediction_data(data, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"Prepare data for link prediction task\"\"\"\n",
    "    edge_index = data.edge_index\n",
    "\n",
    "    # Make edges undirected and unique\n",
    "    row, col = edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    n_edges = row.size(0)\n",
    "\n",
    "    # Random permutation\n",
    "    perm = torch.randperm(n_edges)\n",
    "    row, col = row[perm], col[perm]\n",
    "\n",
    "    # Split sizes\n",
    "    n_val = int(n_edges * val_ratio)\n",
    "    n_test = int(n_edges * test_ratio)\n",
    "\n",
    "    # Create splits\n",
    "    val_edges_pos = torch.stack([row[:n_val], col[:n_val]], dim=0)\n",
    "    test_edges_pos = torch.stack(\n",
    "        [row[n_val : n_val + n_test], col[n_val : n_val + n_test]], dim=0\n",
    "    )\n",
    "    train_edges_pos = torch.stack([row[n_val + n_test :], col[n_val + n_test :]], dim=0)\n",
    "\n",
    "    # Training edges (all except test)\n",
    "    train_edge_index = torch.cat([train_edges_pos, val_edges_pos], dim=1)\n",
    "\n",
    "    print(\"Link Prediction Data Split:\")\n",
    "    print(f\"Training edges: {train_edge_index.size(1)}\")\n",
    "    print(f\"Validation positive edges: {val_edges_pos.size(1)}\")\n",
    "    print(f\"Test positive edges: {test_edges_pos.size(1)}\")\n",
    "\n",
    "    return {\n",
    "        \"train_edge_index\": train_edge_index,\n",
    "        \"train_edges_pos\": train_edges_pos,\n",
    "        \"val_edges_pos\": val_edges_pos,\n",
    "        \"test_edges_pos\": test_edges_pos,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare link prediction data\n",
    "link_data = prepare_link_prediction_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad32a9",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3034723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classification(model, data, optimizer, criterion, epochs=200):\n",
    "    \"\"\"Train model for node classification\"\"\"\n",
    "    train_losses, val_accs = [], []\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        _, node_logits = model(data.x, data.edge_index)\n",
    "\n",
    "        # Compute loss on training nodes only\n",
    "        loss = criterion(node_logits[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, val_logits = model(data.x, data.edge_index)\n",
    "            val_pred = val_logits[data.val_mask].argmax(dim=1)\n",
    "            val_acc = (val_pred == data.y[data.val_mask]).float().mean().item()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}: Loss={loss.item():.4f}, Val Acc={val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39151e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link_prediction(model, data, link_data, optimizer, criterion, epochs=100):\n",
    "    \"\"\"Train model for link prediction\"\"\"\n",
    "    train_losses = []\n",
    "\n",
    "    # Prepare training data\n",
    "    pos_edges = link_data[\"train_edges_pos\"]\n",
    "\n",
    "    # Create negative samples\n",
    "    neg_edges = negative_sampling(\n",
    "        edge_index=link_data[\"train_edge_index\"],\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=pos_edges.size(1),\n",
    "    ).to(data.x.device)\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    train_edges = torch.cat([pos_edges, neg_edges], dim=1)\n",
    "    train_labels = torch.cat(\n",
    "        [\n",
    "            torch.ones(pos_edges.size(1), device=data.x.device),\n",
    "            torch.zeros(neg_edges.size(1), device=data.x.device),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get embeddings\n",
    "        z, _ = model(data.x, link_data[\"train_edge_index\"])\n",
    "\n",
    "        # Predict links\n",
    "        link_scores = model.predict_links(z, train_edges)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(link_scores, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Link Epoch {epoch+1:03d}: Loss={loss.item():.4f}\")\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, link_data):\n",
    "    \"\"\"Comprehensive evaluation of both tasks\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Node Classification Evaluation\n",
    "        _, node_logits = model(data.x, data.edge_index)\n",
    "        test_pred = node_logits[data.test_mask].argmax(dim=1)\n",
    "        node_acc = (test_pred == data.y[data.test_mask]).float().mean().item()\n",
    "\n",
    "        # Link Prediction Evaluation\n",
    "        z, _ = model(data.x, link_data[\"train_edge_index\"])\n",
    "\n",
    "        # Positive test edges\n",
    "        pos_edges = link_data[\"test_edges_pos\"]\n",
    "        pos_scores = torch.sigmoid(model.predict_links(z, pos_edges))\n",
    "\n",
    "        # Negative test edges\n",
    "        neg_edges = negative_sampling(\n",
    "            edge_index=torch.cat(\n",
    "                [link_data[\"train_edge_index\"], link_data[\"val_edges_pos\"]], dim=1\n",
    "            ),\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=pos_edges.size(1),\n",
    "        ).to(data.x.device)\n",
    "        neg_scores = torch.sigmoid(model.predict_links(z, neg_edges))\n",
    "\n",
    "        # Combine predictions\n",
    "        all_scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
    "        all_labels = (\n",
    "            torch.cat([torch.ones(pos_scores.size(0)), torch.zeros(neg_scores.size(0))])\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        # Compute metrics\n",
    "        auc = roc_auc_score(all_labels, all_scores)\n",
    "        ap = average_precision_score(all_labels, all_scores)\n",
    "\n",
    "        # Precision@k\n",
    "        k = min(100, len(all_scores) // 2)\n",
    "        top_k_idx = np.argsort(all_scores)[-k:]\n",
    "        precision_at_k = all_labels[top_k_idx].mean()\n",
    "\n",
    "    return {\n",
    "        \"node_accuracy\": node_acc,\n",
    "        \"link_auc\": auc,\n",
    "        \"link_ap\": ap,\n",
    "        \"link_precision_at_k\": precision_at_k,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24604113",
   "metadata": {},
   "source": [
    "## 7. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint_model(model, data, link_data, epochs_node=100, epochs_link=50):\n",
    "    \"\"\"Complete training pipeline for joint model\"\"\"\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    for key in link_data.keys():\n",
    "        link_data[key] = link_data[key].to(device)\n",
    "\n",
    "    # Phase 1: Node Classification\n",
    "    print(\"\\nPhase 1: Training Node Classification\\n\")\n",
    "\n",
    "    optimizer1 = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    criterion1 = nn.CrossEntropyLoss()\n",
    "\n",
    "    node_losses, node_accs = train_node_classification(\n",
    "        model, data, optimizer1, criterion1, epochs_node\n",
    "    )\n",
    "\n",
    "    # Phase 2: Link Prediction\n",
    "    print(\"\\nPhase 2: Training Link Prediction\\n\")\n",
    "\n",
    "    optimizer2 = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion2 = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    link_losses = train_link_prediction(\n",
    "        model, data, link_data, optimizer2, criterion2, epochs_link\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nFinal evaluation\\n\")\n",
    "\n",
    "    metrics = evaluate_model(model, data, link_data)\n",
    "\n",
    "    print(f\"Node Classification Accuracy: {metrics['node_accuracy']:.4f}\")\n",
    "    print(f\"Link Prediction AUC-ROC: {metrics['link_auc']:.4f}\")\n",
    "    print(f\"Link Prediction Average Precision: {metrics['link_ap']:.4f}\")\n",
    "    print(f\"Link Prediction Precision@100: {metrics['link_precision_at_k']:.4f}\")\n",
    "\n",
    "    return node_losses, node_accs, link_losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6437c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "node_losses, node_accs, link_losses, metrics = train_joint_model(\n",
    "    model, data, link_data, epochs_node=100, epochs_link=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df254cf",
   "metadata": {},
   "source": [
    "## 8. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68379d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(node_losses, node_accs, link_losses, metrics):\n",
    "    \"\"\"Plot training results and metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Node classification loss\n",
    "    axes[0, 0].plot(node_losses, color=\"tab:blue\", linewidth=2)\n",
    "    axes[0, 0].set_title(\"Node Classification Loss\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 0].set_xlabel(\"Epoch\")\n",
    "    axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Node classification accuracy\n",
    "    axes[0, 1].plot(node_accs, color=\"tab:green\", linewidth=2)\n",
    "    axes[0, 1].set_title(\n",
    "        \"Node Classification Validation Accuracy\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, 1].set_xlabel(\"Epoch\")\n",
    "    axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Link prediction loss\n",
    "    axes[0, 2].plot(link_losses, color=\"tab:red\", linewidth=2)\n",
    "    axes[0, 2].set_title(\"Link Prediction Loss\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 2].set_xlabel(\"Epoch\")\n",
    "    axes[0, 2].set_ylabel(\"Loss\")\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Metrics comparison\n",
    "    metric_names = [\"Node Accuracy\", \"Link AUC\", \"Link AP\"]\n",
    "    metric_values = [metrics[\"node_accuracy\"], metrics[\"link_auc\"], metrics[\"link_ap\"]]\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\"]\n",
    "\n",
    "    bars = axes[1, 0].bar(metric_names, metric_values, color=colors)\n",
    "    axes[1, 0].set_title(\"Model Performance Metrics\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 0].set_ylabel(\"Score\")\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{value:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Confusion matrix for node classification\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, node_logits = model(data.x, data.edge_index)\n",
    "        test_pred = node_logits[data.test_mask].argmax(dim=1).cpu().numpy()\n",
    "        test_true = data.y[data.test_mask].cpu().numpy()\n",
    "\n",
    "    cm = confusion_matrix(test_true, test_pred)\n",
    "\n",
    "    im = axes[1, 1].imshow(cm, cmap=\"Blues\", aspect=\"auto\")\n",
    "    axes[1, 1].set_title(\n",
    "        \"Confusion Matrix (Node Classification)\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Predicted\")\n",
    "    axes[1, 1].set_ylabel(\"True\")\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "    # Link prediction scores distribution\n",
    "    with torch.no_grad():\n",
    "        z, _ = model(data.x, link_data[\"train_edge_index\"])\n",
    "        pos_edges = link_data[\"test_edges_pos\"]\n",
    "        neg_edges = negative_sampling(\n",
    "            edge_index=torch.cat(\n",
    "                [link_data[\"train_edge_index\"], link_data[\"val_edges_pos\"]], dim=1\n",
    "            ),\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=pos_edges.size(1),\n",
    "        )\n",
    "\n",
    "        pos_scores = torch.sigmoid(model.predict_links(z, pos_edges)).cpu().numpy()\n",
    "        neg_scores = torch.sigmoid(model.predict_links(z, neg_edges)).cpu().numpy()\n",
    "\n",
    "    axes[1, 2].hist(\n",
    "        pos_scores, bins=30, alpha=0.7, label=\"Positive edges\", color=\"green\"\n",
    "    )\n",
    "    axes[1, 2].hist(neg_scores, bins=30, alpha=0.7, label=\"Negative edges\", color=\"red\")\n",
    "    axes[1, 2].set_title(\n",
    "        \"Link Prediction Scores Distribution\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1, 2].set_xlabel(\"Prediction Score\")\n",
    "    axes[1, 2].set_ylabel(\"Count\")\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"images/training_results_comprehensive.png\", dpi=150, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot results\n",
    "plot_training_results(node_losses, node_accs, link_losses, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e9b96",
   "metadata": {},
   "source": [
    "## 9. Model Interpretation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_node_embeddings(model, data):\n",
    "    \"\"\"Visualize learned node embeddings using t-SNE\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        embeddings = model.get_embeddings(data.x, data.edge_index).cpu().numpy()\n",
    "        labels = data.y.cpu().numpy()\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        embeddings_2d[:, 0],\n",
    "        embeddings_2d[:, 1],\n",
    "        c=labels,\n",
    "        cmap=\"tab20\",\n",
    "        alpha=0.6,\n",
    "        s=10,\n",
    "    )\n",
    "\n",
    "    plt.title(\"t-SNE Visualization of Node Embeddings\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "\n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"w\",\n",
    "            markerfacecolor=plt.cm.tab20(i / 7),\n",
    "            markersize=10,\n",
    "            label=class_names[i],\n",
    "        )\n",
    "        for i in range(7)\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=legend_elements,\n",
    "        title=\"Research Topics\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"images/tsne_embeddings.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return embeddings_2d\n",
    "\n",
    "\n",
    "# Visualize embeddings\n",
    "embeddings_2d = visualize_node_embeddings(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625911f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_predictions(model, data, link_data, num_examples=5):\n",
    "    \"\"\"Analyze and explain model predictions\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    print(\"Model prediction analysis:\")\n",
    "\n",
    "    # Node classification examples\n",
    "    print(\"\\nNode Classification Examples:\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, node_logits = model(data.x, data.edge_index)\n",
    "        predictions = node_logits.argmax(dim=1)\n",
    "        probabilities = F.softmax(node_logits, dim=1)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        true_class = data.y[i].item()\n",
    "        pred_class = predictions[i].item()\n",
    "        confidence = probabilities[i][pred_class].item()\n",
    "\n",
    "        print(f\"Paper {i}:\")\n",
    "        print(f\"  True topic: {class_names[true_class]}\")\n",
    "        print(f\"  Predicted: {class_names[pred_class]} (confidence: {confidence:.2%})\")\n",
    "        print(f\"  {'✓ CORRECT' if true_class == pred_class else '✗ WRONG'}\")\n",
    "        print()\n",
    "\n",
    "    # Link prediction examples\n",
    "    print(\"\\nLink Prediction Examples:\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z, _ = model(data.x, link_data[\"train_edge_index\"])\n",
    "\n",
    "        # Test some specific paper pairs\n",
    "        test_pairs = torch.tensor(\n",
    "            [\n",
    "                [0, 10, 50, 100, 200],  # Source papers\n",
    "                [5, 15, 55, 105, 205],  # Target papers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        scores = torch.sigmoid(model.predict_links(z, test_pairs))\n",
    "\n",
    "    for i in range(test_pairs.size(1)):\n",
    "        src = test_pairs[0, i].item()\n",
    "        dst = test_pairs[1, i].item()\n",
    "        score = scores[i].item()\n",
    "\n",
    "        src_topic = class_names[data.y[src].item()]\n",
    "        dst_topic = class_names[data.y[dst].item()]\n",
    "\n",
    "        print(f\"Citation from Paper {src} ({src_topic}) to Paper {dst} ({dst_topic}):\")\n",
    "        print(f\"  Prediction score: {score:.3f}\")\n",
    "        print(\n",
    "            f\"  Interpretation: {'Likely citation' if score > 0.5 else 'Unlikely citation'}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "\n",
    "# Analyze predictions\n",
    "analyze_model_predictions(model, data, link_data, num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80343dd0",
   "metadata": {},
   "source": [
    "# 10. Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_citations(model, data, link_data, paper_id, top_k=5):\n",
    "    \"\"\"Recommend citations for a given paper\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        z, _ = model(data.x, link_data[\"train_edge_index\"])\n",
    "\n",
    "        # Get paper embedding\n",
    "        paper_embedding = z[paper_id]\n",
    "\n",
    "        # Calculate similarity with all other papers\n",
    "        similarities = F.cosine_similarity(paper_embedding.unsqueeze(0), z, dim=1)\n",
    "\n",
    "        # Get existing citations\n",
    "        existing_citations = data.edge_index[1][data.edge_index[0] == paper_id]\n",
    "\n",
    "        # Find top candidates (excluding existing citations and self)\n",
    "        candidates = []\n",
    "        for i in range(data.num_nodes):\n",
    "            if i != paper_id and i not in existing_citations:\n",
    "                candidates.append((i, similarities[i].item()))\n",
    "\n",
    "        # Sort by similarity\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_candidates = candidates[:top_k]\n",
    "\n",
    "    print(f\"\\nCitation Recommendations for Paper {paper_id}:\")\n",
    "    print(f\"Topic: {class_names[data.y[paper_id].item()]}\\n\")\n",
    "\n",
    "    for rank, (cand_id, similarity) in enumerate(top_candidates, 1):\n",
    "        cand_topic = class_names[data.y[cand_id].item()]\n",
    "        print(f\"{rank}. Paper {cand_id} ({cand_topic})\")\n",
    "        print(f\"   Similarity score: {similarity:.3f}\")\n",
    "        print(\n",
    "            f\"   Potential citation strength: {'High' if similarity > 0.5 else 'Medium'}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    return top_candidates\n",
    "\n",
    "\n",
    "# Example: Get citation recommendations for paper 42\n",
    "recommendations = recommend_citations(model, data, link_data, paper_id=42, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555fe601",
   "metadata": {},
   "source": [
    "## 11. Advanced: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(data, link_data):\n",
    "    \"\"\"Experiment with different hyperparameters\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Different hidden dimensions to try\n",
    "    hidden_dims = [64, 128, 256]\n",
    "    learning_rates = [0.01, 0.005, 0.001]\n",
    "\n",
    "    print(\"Hyperparameter Tuning\\n\")\n",
    "\n",
    "    for hidden_dim in hidden_dims:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\nTesting: hidden_dim={hidden_dim}, lr={lr}\\n\")\n",
    "\n",
    "            # Initialize new model\n",
    "            model_tune = JointGraphModel(\n",
    "                in_channels=data.num_features,\n",
    "                hidden_channels=hidden_dim,\n",
    "                num_classes=dataset.num_classes,\n",
    "            ).to(data.x.device)\n",
    "\n",
    "            # Train\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model_tune.parameters(), lr=lr, weight_decay=5e-4\n",
    "            )\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Quick training for tuning\n",
    "            train_losses, _ = train_node_classification(\n",
    "                model_tune, data, optimizer, criterion, epochs=50\n",
    "            )\n",
    "\n",
    "            # Evaluate\n",
    "            metrics = evaluate_model(model_tune, data, link_data)\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"hidden_dim\": hidden_dim,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"node_accuracy\": metrics[\"node_accuracy\"],\n",
    "                    \"link_auc\": metrics[\"link_auc\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"Results: Acc={metrics['node_accuracy']:.3f}, AUC={metrics['link_auc']:.3f}\"\n",
    "            )\n",
    "\n",
    "    # Find best configuration\n",
    "    best_result = max(results, key=lambda x: x[\"node_accuracy\"])\n",
    "\n",
    "    print(\"Best configuration:\")\n",
    "    print(f\"Hidden dimensions: {best_result['hidden_dim']}\")\n",
    "    print(f\"Learning rate: {best_result['learning_rate']}\")\n",
    "    print(f\"Node accuracy: {best_result['node_accuracy']:.3f}\")\n",
    "    print(f\"Link AUC: {best_result['link_auc']:.3f}\")\n",
    "\n",
    "    return results, best_result\n",
    "\n",
    "tuning_results, best_config = hyperparameter_tuning(data, link_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e981a9",
   "metadata": {},
   "source": [
    "## 12. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path=\"joint_graph_model.pth\"):\n",
    "    \"\"\"Save model to file\"\"\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"model_config\": {\n",
    "                \"in_channels\": data.num_features,\n",
    "                \"hidden_channels\": 128,\n",
    "                \"num_classes\": dataset.num_classes,\n",
    "            },\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "\n",
    "def load_model(path=\"joint_graph_model.pth\"):\n",
    "    \"\"\"Load model from file\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    model = JointGraphModel(\n",
    "        in_channels=checkpoint[\"model_config\"][\"in_channels\"],\n",
    "        hidden_channels=checkpoint[\"model_config\"][\"hidden_channels\"],\n",
    "        num_classes=checkpoint[\"model_config\"][\"num_classes\"],\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Model loaded from {path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "save_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
