{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde2871c",
   "metadata": {},
   "source": [
    "# Tutorial: Node Classification and Link Prediction in Citation Networks\n",
    "\n",
    "## Applying Graph Machine Learning to Academic Paper Analysis\n",
    "\n",
    "**Authors**: Denis Troegubov  \n",
    "**Date**: December 2025  \n",
    "**GitHub**: [Link](https://github.com/BogGoro/predicting-paper-topics-and-connections-tutorial)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this hands-on tutorial on Graph Machine Learning! In this notebook, we'll explore how to apply state-of-the-art Graph Neural Networks (GNNs) to solve two fundamental problems in citation networks:\n",
    "\n",
    "1. **Node Classification**: Predicting research topics of academic papers\n",
    "2. **Link Prediction**: Recommending potential citations between papers\n",
    "\n",
    "We'll use the **Cora dataset** - a classic benchmark in graph ML - and implement our models using **PyTorch Geometric (PyG)**, the leading library for deep learning on graphs.\n",
    "\n",
    "### Why This Tutorial?\n",
    "- **Practical Approach**: We focus on real-world applications rather than theory\n",
    "- **Code-First**: Learn by implementing working models\n",
    "- **Production-Ready**: Code structured for reproducibility and extension\n",
    "\n",
    "### Prerequisites\n",
    "- Basic knowledge of Python and PyTorch\n",
    "- Understanding of neural networks\n",
    "- No prior experience with graphs required!\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f523f2c",
   "metadata": {},
   "source": [
    "1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d73e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch Geometric if you are running in colab\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import SAGEConv, GCNConv\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create directory for visualizations\n",
    "try:\n",
    "    os.mkdir(\"images\")\n",
    "except Exception:\n",
    "    print(\"Directory already exists\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10667a9c",
   "metadata": {},
   "source": [
    "## 2. Understanding the Data: Cora Dataset\n",
    "\n",
    "### What is Cora?\n",
    "The Cora dataset is a classic citation network consisting of machine learning papers. It's widely used as a benchmark in graph ML research.\n",
    "\n",
    "### Dataset Statistics:\n",
    "- **Nodes**: 2,708 academic papers\n",
    "- **Edges**: 10,556 citation links (directed)\n",
    "- **Features**: 1,433-dimensional binary word vectors (bag-of-words)\n",
    "- **Classes**: 7 research topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root=\"/tmp/Cora\", name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Number of graphs: {len(dataset)}\")\n",
    "print(f\"Number of nodes: {data.num_nodes}\")\n",
    "print(f\"Number of edges: {data.num_edges}\")\n",
    "print(f\"Number of features: {data.num_features}\")\n",
    "print(f\"Number of classes: {dataset.num_classes}\")\n",
    "print(f\"Has isolated nodes: {data.has_isolated_nodes()}\")\n",
    "print(f\"Has self-loops: {data.has_self_loops()}\")\n",
    "print(f\"Is undirected: {data.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ca72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class distribution\n",
    "class_names = [\n",
    "    \"Case-Based\",\n",
    "    \"Genetic Algorithms\",\n",
    "    \"Neural Networks\",\n",
    "    \"Probabilistic Methods\",\n",
    "    \"Reinforcement Learning\",\n",
    "    \"Rule Learning\",\n",
    "    \"Theory\",\n",
    "]\n",
    "\n",
    "class_counts = torch.bincount(data.y).numpy()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(class_names, class_counts, color=sns.color_palette(\"husl\", 7))\n",
    "plt.title(\n",
    "    \"Distribution of Paper Topics in Cora Dataset\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "plt.xlabel(\"Research Topic\", fontsize=12)\n",
    "plt.ylabel(\"Number of Papers\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar, count in zip(bars, class_counts):\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + 5,\n",
    "        f\"{count}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/class_distribution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a20da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a subgraph of the citation network\n",
    "def visualize_citation_subgraph(data, num_nodes=100):\n",
    "    \"\"\"Visualize a small subgraph of the citation network\"\"\"\n",
    "    # Take first num_nodes nodes\n",
    "    subgraph_nodes = torch.arange(num_nodes)\n",
    "\n",
    "    # Create mask for edges between these nodes\n",
    "    mask = (data.edge_index[0] < num_nodes) & (data.edge_index[1] < num_nodes)\n",
    "    subgraph_edges = data.edge_index[:, mask]\n",
    "\n",
    "    # Create subgraph\n",
    "    subgraph = Data(\n",
    "        x=data.x[:num_nodes], edge_index=subgraph_edges, y=data.y[:num_nodes]\n",
    "    )\n",
    "\n",
    "    # Convert to NetworkX for visualization\n",
    "    G = to_networkx(subgraph, to_undirected=True)\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Node colors by class\n",
    "    node_colors = [data.y[i].item() for i in range(num_nodes)]\n",
    "\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, node_size=50, node_color=node_colors, cmap=plt.cm.Set2, alpha=0.8\n",
    "    )\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2, width=0.5)\n",
    "\n",
    "    plt.title(\n",
    "        f\"Citation Network Subgraph (First {num_nodes} Papers)\",\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Create legend for classes\n",
    "    legend_elements = [\n",
    "        plt.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"w\",\n",
    "            markerfacecolor=plt.cm.Set2(i / 7),\n",
    "            markersize=10,\n",
    "            label=class_names[i],\n",
    "        )\n",
    "        for i in range(7)\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=legend_elements,\n",
    "        title=\"Research Topics\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"images/citation_subgraph.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# Visualize subgraph\n",
    "G = visualize_citation_subgraph(data, num_nodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcdc8b",
   "metadata": {},
   "source": [
    "## 3. Graph Neural Networks: Core Concepts\n",
    "\n",
    "### What are GNNs?\n",
    "Graph Neural Networks extend deep learning to graph-structured data. Unlike CNNs for images or RNNs for sequences, GNNs can handle arbitrary graph structures.\n",
    "\n",
    "### Key Idea: Message Passing\n",
    "GNNs work by **aggregating information from neighbors**. Each layer:\n",
    "1. **Gathers** neighbor features\n",
    "2. **Aggregates** them (sum, mean, max)\n",
    "3. **Updates** node representations\n",
    "\n",
    "### Why GraphSAGE?\n",
    "We'll use **GraphSAGE** (SAmple and aggreGatE) because:\n",
    "- **Inductive learning**: Can generalize to unseen nodes\n",
    "- **Scalability**: Works with large graphs via neighborhood sampling\n",
    "- **Flexibility**: Can use different aggregation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f203",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "We'll implement **separate models** for node classification and link prediction to prevent task interference:\n",
    "\n",
    "### Model Components:\n",
    "\n",
    "```\n",
    "Node Classification System:\n",
    "├── NodeEncoder\n",
    "└── NodeClassifier\n",
    "\n",
    "Link Prediction System:\n",
    "├── LinkEncoder\n",
    "└── LinkPredictor\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c2bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEncoder(nn.Module):\n",
    "    \"\"\"Encoder specifically for node classification\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb3e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkEncoder(nn.Module):\n",
    "    \"\"\"Encoder specifically optimized for link prediction\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # First SAGE layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Second SAGE layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get intermediate node embeddings\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.conv1(x, edge_index)\n",
    "            embeddings = F.relu(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEClassifier(nn.Module):\n",
    "    \"\"\"GraphSAGE with focal loss for handling class imbalance\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_channels, num_classes, dropout=0.6, gamma=2.0, alpha=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sage1 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.sage2 = SAGEConv(hidden_channels, hidden_channels // 2)\n",
    "        self.sage3 = SAGEConv(hidden_channels // 2, num_classes)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels // 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Focal loss parameters\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha  # Can be list of per-class weights\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.bn1(self.sage1(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.sage2(x, edge_index)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.sage3(x, edge_index)\n",
    "        return x  # Return logits\n",
    "\n",
    "    def focal_loss(self, logits, labels):\n",
    "        \"\"\"Focal loss for imbalanced datasets\"\"\"\n",
    "        ce_loss = F.cross_entropy(logits, labels, reduction=\"none\")\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            alpha = self.alpha.to(logits.device)\n",
    "            alpha_weight = alpha[labels]\n",
    "            focal_loss = alpha_weight * focal_loss\n",
    "\n",
    "        return focal_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3544c918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Link Predictor\n",
    "class LinkPredictor(nn.Module):\n",
    "    \"\"\"MLP for predicting links between nodes\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(in_channels * 2, hidden_channels)\n",
    "        self.lin2 = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        # Get embeddings for source and destination nodes\n",
    "        src = z[edge_index[0]]\n",
    "        dst = z[edge_index[1]]\n",
    "\n",
    "        # Concatenate and predict\n",
    "        x = torch.cat([src, dst], dim=1)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the graph models\n",
    "class GraphModels(nn.Module):\n",
    "    \"\"\"Container for node classification and link prediction models\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super().__init__()\n",
    "        # Separate encoders for each task\n",
    "        self.node_encoder = NodeEncoder(in_channels, hidden_channels, hidden_channels)\n",
    "        self.link_encoder = LinkEncoder(in_channels, hidden_channels, hidden_channels)\n",
    "\n",
    "        # Task-specific heads\n",
    "        self.node_classifier = GraphSAGEClassifier(hidden_channels, num_classes)\n",
    "        self.link_predictor = LinkPredictor(hidden_channels, hidden_channels // 2)\n",
    "\n",
    "    def forward_node(self, x, edge_index):\n",
    "        \"\"\"Forward pass for node classification\"\"\"\n",
    "        z = self.node_encoder(x, edge_index)\n",
    "        node_logits = self.node_classifier(z, edge_index)\n",
    "        return z, node_logits\n",
    "\n",
    "    def forward_link(self, x, edge_index, edge_label_index=None):\n",
    "        \"\"\"Forward pass for link prediction\"\"\"\n",
    "        z = self.link_encoder(x, edge_index)\n",
    "        if edge_label_index is None:\n",
    "            edge_label_index = edge_index\n",
    "        link_pred = self.link_predictor(z, edge_label_index)\n",
    "        return z, link_pred\n",
    "\n",
    "    def predict_links(self, x, edge_index, link_edge_index=None):\n",
    "        \"\"\"Predict links using link encoder\"\"\"\n",
    "        with torch.no_grad():\n",
    "            z = self.link_encoder(x, edge_index)\n",
    "            if link_edge_index is None:\n",
    "                link_edge_index = edge_index\n",
    "            link_pred = self.link_predictor(z, link_edge_index)\n",
    "        return link_pred\n",
    "\n",
    "    def get_node_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get embeddings from node encoder\"\"\"\n",
    "        with torch.no_grad():\n",
    "            z = self.node_encoder(x, edge_index)\n",
    "        return z\n",
    "\n",
    "    def get_link_embeddings(self, x, edge_index):\n",
    "        \"\"\"Get embeddings from link encoder\"\"\"\n",
    "        with torch.no_grad():\n",
    "            z = self.link_encoder(x, edge_index)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c45ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = GraphModels(\n",
    "    in_channels=data.num_features, hidden_channels=128, num_classes=dataset.num_classes\n",
    ")\n",
    "\n",
    "print(\"Models initialized successfully!\")\n",
    "print(\n",
    "    f\"Node encoder parameters: {sum(p.numel() for p in models.node_encoder.parameters()):,}\"\n",
    ")\n",
    "print(\n",
    "    f\"Link encoder parameters: {sum(p.numel() for p in models.link_encoder.parameters()):,}\"\n",
    ")\n",
    "print(\n",
    "    f\"Node classifier parameters: {sum(p.numel() for p in models.node_classifier.parameters()):,}\"\n",
    ")\n",
    "print(\n",
    "    f\"Link predictor parameters: {sum(p.numel() for p in models.link_predictor.parameters()):,}\"\n",
    ")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in models.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b271b",
   "metadata": {},
   "source": [
    "## 5. Data Preparation for Link Prediction\n",
    "\n",
    "For link prediction, we need to:\n",
    "1. Split edges into train/val/test sets\n",
    "2. Create negative examples (non-existent edges)\n",
    "3. Balance positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d03f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_link_prediction_data(data, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"Prepare data for link prediction task\"\"\"\n",
    "    edge_index = data.edge_index\n",
    "\n",
    "    # Make edges undirected and unique\n",
    "    row, col = edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    n_edges = row.size(0)\n",
    "\n",
    "    # Random permutation\n",
    "    perm = torch.randperm(n_edges)\n",
    "    row, col = row[perm], col[perm]\n",
    "\n",
    "    # Split sizes\n",
    "    n_val = int(n_edges * val_ratio)\n",
    "    n_test = int(n_edges * test_ratio)\n",
    "\n",
    "    # Create splits\n",
    "    val_edges_pos = torch.stack([row[:n_val], col[:n_val]], dim=0)\n",
    "    test_edges_pos = torch.stack(\n",
    "        [row[n_val : n_val + n_test], col[n_val : n_val + n_test]], dim=0\n",
    "    )\n",
    "    train_edges_pos = torch.stack([row[n_val + n_test :], col[n_val + n_test :]], dim=0)\n",
    "\n",
    "    # Training edges (all except test)\n",
    "    train_edge_index = torch.cat([train_edges_pos, val_edges_pos], dim=1)\n",
    "\n",
    "    print(\"Link Prediction Data Split:\")\n",
    "    print(f\"Training edges: {train_edge_index.size(1)}\")\n",
    "    print(f\"Validation positive edges: {val_edges_pos.size(1)}\")\n",
    "    print(f\"Test positive edges: {test_edges_pos.size(1)}\")\n",
    "\n",
    "    return {\n",
    "        \"train_edge_index\": train_edge_index,\n",
    "        \"train_edges_pos\": train_edges_pos,\n",
    "        \"val_edges_pos\": val_edges_pos,\n",
    "        \"test_edges_pos\": test_edges_pos,\n",
    "    }\n",
    "\n",
    "\n",
    "# Prepare link prediction data\n",
    "link_data = prepare_link_prediction_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad32a9",
   "metadata": {},
   "source": [
    "## 6. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3034723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node_classification(models, data, epochs=200):\n",
    "    \"\"\"Train the node classification model\"\"\"\n",
    "    print(\"\\nTraining Node Classification Model\\n\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    models = models.to(device)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    # Optimizer for node-related parameters only\n",
    "    node_params = list(models.node_encoder.parameters()) + list(models.node_classifier.parameters())\n",
    "    optimizer = torch.optim.Adam(node_params, lr=0.01, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses, val_accs = [], []\n",
    "    best_val_acc = 0\n",
    "    best_state = {\n",
    "        'encoder': None,\n",
    "        'classifier': None\n",
    "    }\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        models.node_encoder.train()\n",
    "        models.node_classifier.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward through node models only\n",
    "        z, node_logits = models.forward_node(data.x, data.edge_index)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(node_logits[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        models.node_encoder.eval()\n",
    "        models.node_classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            _, node_logits = models.forward_node(data.x, data.edge_index)\n",
    "            val_pred = node_logits[data.val_mask].argmax(dim=1)\n",
    "            val_acc = (val_pred == data.y[data.val_mask]).float().mean().item()\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_state['encoder'] = {k: v.clone() for k, v in models.node_encoder.state_dict().items()}\n",
    "                best_state['classifier'] = {k: v.clone() for k, v in models.node_classifier.state_dict().items()}\n",
    "            \n",
    "            train_pred = node_logits[data.train_mask].argmax(dim=1)\n",
    "            train_acc = (train_pred == data.y[data.train_mask]).float().mean().item()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        val_accs.append(val_acc)  # Now val_acc is always defined\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1:03d}: Loss={loss:.4f}, \"\n",
    "                  f\"Train Acc={train_acc:.4f}, Val Acc={val_acc:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_state['encoder']:\n",
    "        models.node_encoder.load_state_dict(best_state['encoder'])\n",
    "        models.node_classifier.load_state_dict(best_state['classifier'])\n",
    "    \n",
    "    print(f\"Node training completed. Best Val Acc: {best_val_acc:.4f}\")\n",
    "    return models, train_losses, val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39151e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_link_prediction(models, data, link_data, epochs=100):\n",
    "    \"\"\"Train only the link prediction model\"\"\"\n",
    "    print(\"\\nTraining Link Prediction Model\\n\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    models = models.to(device)\n",
    "    data = data.to(device)\n",
    "\n",
    "    for key in link_data.keys():\n",
    "        link_data[key] = link_data[key].to(device)\n",
    "\n",
    "    # Optimizer for link-related parameters only\n",
    "    link_params = list(models.link_encoder.parameters()) + list(\n",
    "        models.link_predictor.parameters()\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(link_params, lr=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Prepare training data\n",
    "    pos_edges = link_data[\"train_edges_pos\"]\n",
    "    neg_edges = negative_sampling(\n",
    "        edge_index=link_data[\"train_edge_index\"],\n",
    "        num_nodes=data.num_nodes,\n",
    "        num_neg_samples=pos_edges.size(1),\n",
    "    ).to(device)\n",
    "\n",
    "    # Combine positive and negative edges\n",
    "    train_edges = torch.cat([pos_edges, neg_edges], dim=1)\n",
    "    train_labels = torch.cat(\n",
    "        [\n",
    "            torch.ones(pos_edges.size(1), device=device),\n",
    "            torch.zeros(neg_edges.size(1), device=device),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        models.link_encoder.train()\n",
    "        models.link_predictor.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward through link models only\n",
    "        z, link_scores = models.forward_link(\n",
    "            data.x, link_data[\"train_edge_index\"], train_edges\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(link_scores, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Link Epoch {epoch+1:03d}: Loss={loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Link training completed.\")\n",
    "    return models, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, data, link_data):\n",
    "    \"\"\"Comprehensive evaluation of both models\"\"\"\n",
    "    models.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Node Classification Evaluation (using node encoder)\n",
    "        _, node_logits = models.forward_node(data.x, data.edge_index)\n",
    "        test_pred = node_logits[data.test_mask].argmax(dim=1)\n",
    "        node_acc = (test_pred == data.y[data.test_mask]).float().mean().item()\n",
    "\n",
    "        # Link Prediction Evaluation (using link encoder)\n",
    "        z, _ = models.forward_link(data.x, link_data[\"train_edge_index\"])\n",
    "\n",
    "        # Positive test edges\n",
    "        pos_edges = link_data[\"test_edges_pos\"]\n",
    "        pos_scores = torch.sigmoid(models.link_predictor(z, pos_edges))\n",
    "\n",
    "        # Negative test edges\n",
    "        neg_edges = negative_sampling(\n",
    "            edge_index=torch.cat(\n",
    "                [link_data[\"train_edge_index\"], link_data[\"val_edges_pos\"]], dim=1\n",
    "            ),\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=pos_edges.size(1),\n",
    "        ).to(data.x.device)\n",
    "        neg_scores = torch.sigmoid(models.link_predictor(z, neg_edges))\n",
    "\n",
    "        # Combine predictions\n",
    "        all_scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
    "        all_labels = (\n",
    "            torch.cat([torch.ones(pos_scores.size(0)), torch.zeros(neg_scores.size(0))])\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        # Compute metrics\n",
    "        auc = roc_auc_score(all_labels, all_scores)\n",
    "        ap = average_precision_score(all_labels, all_scores)\n",
    "\n",
    "        # Precision@k\n",
    "        k = min(100, len(all_scores) // 2)\n",
    "        top_k_idx = np.argsort(all_scores)[-k:]\n",
    "        precision_at_k = all_labels[top_k_idx].mean()\n",
    "\n",
    "    return {\n",
    "        \"node_accuracy\": node_acc,\n",
    "        \"link_auc\": auc,\n",
    "        \"link_ap\": ap,\n",
    "        \"link_precision_at_k\": precision_at_k,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24604113",
   "metadata": {},
   "source": [
    "## 7. Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(models, data, link_data, epochs_node=100, epochs_link=50):\n",
    "    \"\"\"Complete training pipeline for separate models\"\"\"\n",
    "    print(\"Starting training with separate models...\")\n",
    "\n",
    "    # Phase 1: Train node classification\n",
    "    models, node_losses, node_accs = train_node_classification(\n",
    "        models, data, epochs_node\n",
    "    )\n",
    "\n",
    "    # Phase 2: Train link prediction\n",
    "    models, link_losses = train_link_prediction(\n",
    "        models, data, link_data, epochs_link\n",
    "    )\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"\\nFinal Evaluation\\n\")\n",
    "\n",
    "    metrics = evaluate_models(models, data, link_data)\n",
    "\n",
    "    print(f\"Node Classification Accuracy: {metrics['node_accuracy']:.4f}\")\n",
    "    print(f\"Link Prediction AUC-ROC: {metrics['link_auc']:.4f}\")\n",
    "    print(f\"Link Prediction Average Precision: {metrics['link_ap']:.4f}\")\n",
    "    print(f\"Link Prediction Precision@100: {metrics['link_precision_at_k']:.4f}\")\n",
    "\n",
    "    return models, node_losses, node_accs, link_losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6437c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "models, node_losses, node_accs, link_losses, metrics = train_models(\n",
    "    models, data, link_data, epochs_node=50, epochs_link=150\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df254cf",
   "metadata": {},
   "source": [
    "## 8. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68379d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(node_losses, node_accs, link_losses, metrics):\n",
    "    \"\"\"Plot training results and metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Node classification loss\n",
    "    axes[0, 0].plot(node_losses, color=\"tab:blue\", linewidth=2)\n",
    "    axes[0, 0].set_title(\"Node Classification Loss\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 0].set_xlabel(\"Epoch\")\n",
    "    axes[0, 0].set_ylabel(\"Loss\")\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Node classification accuracy\n",
    "    axes[0, 1].plot(node_accs, color=\"tab:green\", linewidth=2)\n",
    "    axes[0, 1].set_title(\n",
    "        \"Node Classification Validation Accuracy\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0, 1].set_xlabel(\"Epoch\")\n",
    "    axes[0, 1].set_ylabel(\"Accuracy\")\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Link prediction loss\n",
    "    axes[0, 2].plot(link_losses, color=\"tab:red\", linewidth=2)\n",
    "    axes[0, 2].set_title(\"Link Prediction Loss\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, 2].set_xlabel(\"Epoch\")\n",
    "    axes[0, 2].set_ylabel(\"Loss\")\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Metrics comparison\n",
    "    metric_names = [\"Node Accuracy\", \"Link AUC\", \"Link AP\"]\n",
    "    metric_values = [metrics[\"node_accuracy\"], metrics[\"link_auc\"], metrics[\"link_ap\"]]\n",
    "    colors = [\"tab:blue\", \"tab:orange\", \"tab:green\"]\n",
    "\n",
    "    bars = axes[1, 0].bar(metric_names, metric_values, color=colors)\n",
    "    axes[1, 0].set_title(\"Model Performance Metrics\", fontsize=12, fontweight=\"bold\")\n",
    "    axes[1, 0].set_ylabel(\"Score\")\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, metric_values):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{value:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # Confusion matrix for node classification\n",
    "    models.eval()\n",
    "    with torch.no_grad():\n",
    "        # FIX: Use forward_node instead of direct call\n",
    "        _, node_logits = models.forward_node(data.x, data.edge_index)\n",
    "        test_pred = node_logits[data.test_mask].argmax(dim=1).cpu().numpy()\n",
    "        test_true = data.y[data.test_mask].cpu().numpy()\n",
    "\n",
    "    cm = confusion_matrix(test_true, test_pred)\n",
    "\n",
    "    im = axes[1, 1].imshow(cm, cmap=\"Blues\", aspect=\"auto\")\n",
    "    axes[1, 1].set_title(\n",
    "        \"Confusion Matrix (Node Classification)\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1, 1].set_xlabel(\"Predicted\")\n",
    "    axes[1, 1].set_ylabel(\"True\")\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "    # Link prediction scores distribution\n",
    "    with torch.no_grad():\n",
    "        z, _ = models.forward_link(data.x, link_data[\"train_edge_index\"])\n",
    "        pos_edges = link_data[\"test_edges_pos\"]\n",
    "        neg_edges = negative_sampling(\n",
    "            edge_index=torch.cat(\n",
    "                [link_data[\"train_edge_index\"], link_data[\"val_edges_pos\"]], dim=1\n",
    "            ),\n",
    "            num_nodes=data.num_nodes,\n",
    "            num_neg_samples=pos_edges.size(1),\n",
    "        )\n",
    "\n",
    "        pos_scores = torch.sigmoid(models.link_predictor(z, pos_edges)).cpu().numpy()\n",
    "        neg_scores = torch.sigmoid(models.link_predictor(z, neg_edges)).cpu().numpy()\n",
    "\n",
    "    axes[1, 2].hist(\n",
    "        pos_scores, bins=30, alpha=0.7, label=\"Positive edges\", color=\"green\"\n",
    "    )\n",
    "    axes[1, 2].hist(neg_scores, bins=30, alpha=0.7, label=\"Negative edges\", color=\"red\")\n",
    "    axes[1, 2].set_title(\n",
    "        \"Link Prediction Scores Distribution\", fontsize=12, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1, 2].set_xlabel(\"Prediction Score\")\n",
    "    axes[1, 2].set_ylabel(\"Count\")\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        \"images/training_results_comprehensive.png\", dpi=150, bbox_inches=\"tight\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot results\n",
    "plot_training_results(node_losses, node_accs, link_losses, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e9b96",
   "metadata": {},
   "source": [
    "## 9. Model Interpretation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_node_embeddings_separate(models, data):\n",
    "    \"\"\"Visualize learned node embeddings using t-SNE (from node encoder)\"\"\"\n",
    "    models.node_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings from node encoder\n",
    "        embeddings = models.get_node_embeddings(data.x, data.edge_index).cpu().numpy()\n",
    "        labels = data.y.cpu().numpy()\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        embeddings_2d[:, 0],\n",
    "        embeddings_2d[:, 1],\n",
    "        c=labels,\n",
    "        cmap=\"tab20\",\n",
    "        alpha=0.6,\n",
    "        s=10,\n",
    "    )\n",
    "\n",
    "    plt.title(\n",
    "        \"t-SNE Visualization of Node Encoder Embeddings\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "\n",
    "    # Create legend\n",
    "    legend_elements = [\n",
    "        plt.Line2D(\n",
    "            [0],\n",
    "            [0],\n",
    "            marker=\"o\",\n",
    "            color=\"w\",\n",
    "            markerfacecolor=plt.cm.tab20(i / 7),\n",
    "            markersize=10,\n",
    "            label=class_names[i],\n",
    "        )\n",
    "        for i in range(7)\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=legend_elements,\n",
    "        title=\"Research Topics\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"images/tsne_node_embeddings.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    return embeddings_2d\n",
    "\n",
    "\n",
    "# Use the new visualization function\n",
    "embeddings_2d = visualize_node_embeddings_separate(models, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625911f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_predictions(models, data, link_data, num_examples=5):\n",
    "    \"\"\"Analyze and explain model predictions from models\"\"\"\n",
    "    models.eval()\n",
    "\n",
    "    print(\"Model prediction analysis:\")\n",
    "\n",
    "    # Node classification examples\n",
    "    print(\"\\nNode Classification Examples (using node encoder):\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, node_logits = models.forward_node(data.x, data.edge_index)\n",
    "        predictions = node_logits.argmax(dim=1)\n",
    "        probabilities = F.softmax(node_logits, dim=1)\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        true_class = data.y[i].item()\n",
    "        pred_class = predictions[i].item()\n",
    "        confidence = probabilities[i][pred_class].item()\n",
    "\n",
    "        print(f\"Paper {i}:\")\n",
    "        print(f\"  True topic: {class_names[true_class]}\")\n",
    "        print(f\"  Predicted: {class_names[pred_class]} (confidence: {confidence:.2%})\")\n",
    "        print(f\"  {'CORRECT' if true_class == pred_class else 'WRONG'}\")\n",
    "        print()\n",
    "\n",
    "    # Link prediction examples\n",
    "    print(\"\\nLink Prediction Examples (using link encoder):\\n\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z, _ = models.forward_link(data.x, link_data[\"train_edge_index\"])\n",
    "\n",
    "        # Test some specific paper pairs\n",
    "        test_pairs = torch.tensor(\n",
    "            [\n",
    "                [0, 10, 50, 100, 200],  # Source papers\n",
    "                [5, 15, 55, 105, 205],  # Target papers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        scores = torch.sigmoid(models.link_predictor(z, test_pairs))\n",
    "\n",
    "    for i in range(test_pairs.size(1)):\n",
    "        src = test_pairs[0, i].item()\n",
    "        dst = test_pairs[1, i].item()\n",
    "        score = scores[i].item()\n",
    "\n",
    "        src_topic = class_names[data.y[src].item()]\n",
    "        dst_topic = class_names[data.y[dst].item()]\n",
    "\n",
    "        print(f\"Citation from Paper {src} ({src_topic}) to Paper {dst} ({dst_topic}):\")\n",
    "        print(f\"  Prediction score: {score:.3f}\")\n",
    "        print(\n",
    "            f\"  Interpretation: {'Likely citation' if score > 0.5 else 'Unlikely citation'}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "\n",
    "# Use the new analysis function\n",
    "analyze_model_predictions(models, data, link_data, num_examples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80343dd0",
   "metadata": {},
   "source": [
    "# 10. Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_citations(models, data, link_data, paper_id, top_k=5):\n",
    "    \"\"\"Recommend citations using link encoder\"\"\"\n",
    "    models.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings from link encoder\n",
    "        z, _ = models.forward_link(data.x, link_data[\"train_edge_index\"])\n",
    "\n",
    "        # Get paper embedding\n",
    "        paper_embedding = z[paper_id]\n",
    "\n",
    "        # Calculate similarity with all other papers\n",
    "        similarities = F.cosine_similarity(paper_embedding.unsqueeze(0), z, dim=1)\n",
    "\n",
    "        # Get existing citations\n",
    "        existing_citations = data.edge_index[1][data.edge_index[0] == paper_id]\n",
    "\n",
    "        # Find top candidates (excluding existing citations and self)\n",
    "        candidates = []\n",
    "        for i in range(data.num_nodes):\n",
    "            if i != paper_id and i not in existing_citations:\n",
    "                candidates.append((i, similarities[i].item()))\n",
    "\n",
    "        # Sort by similarity\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_candidates = candidates[:top_k]\n",
    "\n",
    "    print(f\"\\nCitation Recommendations for Paper {paper_id}:\")\n",
    "    print(f\"Topic: {class_names[data.y[paper_id].item()]}\\n\")\n",
    "\n",
    "    for rank, (cand_id, similarity) in enumerate(top_candidates, 1):\n",
    "        cand_topic = class_names[data.y[cand_id].item()]\n",
    "        print(f\"{rank}. Paper {cand_id} ({cand_topic})\")\n",
    "        print(f\"   Similarity score: {similarity:.3f}\")\n",
    "        print(\n",
    "            f\"   Potential citation strength: {'High' if similarity > 0.5 else 'Medium'}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    return top_candidates\n",
    "\n",
    "\n",
    "# Example: Get citation recommendations for paper 42\n",
    "recommendations = recommend_citations(\n",
    "    models, data, link_data, paper_id=42, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e981a9",
   "metadata": {},
   "source": [
    "## 12. Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(models, path=\"graph_models.pth\"):\n",
    "    \"\"\"Save models to file\"\"\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"node_encoder_state\": models.node_encoder.state_dict(),\n",
    "            \"node_classifier_state\": models.node_classifier.state_dict(),\n",
    "            \"link_encoder_state\": models.link_encoder.state_dict(),\n",
    "            \"link_predictor_state\": models.link_predictor.state_dict(),\n",
    "            \"model_config\": {\n",
    "                \"in_channels\": data.num_features,\n",
    "                \"hidden_channels\": 128,\n",
    "                \"num_classes\": dataset.num_classes,\n",
    "            },\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "    print(f\"Models saved to {path}\")\n",
    "\n",
    "\n",
    "def load_models(path=\"graph_models.pth\"):\n",
    "    \"\"\"Load models from file\"\"\"\n",
    "    checkpoint = torch.load(path)\n",
    "\n",
    "    models = GraphModels(\n",
    "        in_channels=checkpoint[\"model_config\"][\"in_channels\"],\n",
    "        hidden_channels=checkpoint[\"model_config\"][\"hidden_channels\"],\n",
    "        num_classes=checkpoint[\"model_config\"][\"num_classes\"],\n",
    "    )\n",
    "\n",
    "    models.node_encoder.load_state_dict(checkpoint[\"node_encoder_state\"])\n",
    "    models.node_classifier.load_state_dict(checkpoint[\"node_classifier_state\"])\n",
    "    models.link_encoder.load_state_dict(checkpoint[\"link_encoder_state\"])\n",
    "    models.link_predictor.load_state_dict(checkpoint[\"link_predictor_state\"])\n",
    "\n",
    "    print(f\"Models loaded from {path}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "# Save the trained models\n",
    "save_models(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
